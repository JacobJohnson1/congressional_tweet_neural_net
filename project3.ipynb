{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup \n",
    "from keras import layers\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>party_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"RT @KUSINews: One of our longtime viewers wa...</td>\n",
       "      <td>KUSI</td>\n",
       "      <td>10</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>b\"Today I'm urging the @CDCgov to immediately ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>111</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Tomorrow, #MO03 seniors graduate from Calvar...</td>\n",
       "      <td>MO03</td>\n",
       "      <td>2</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>b'Congrats to #TeamUSA and Canton Native @JGre...</td>\n",
       "      <td>TeamUSA WorldJuniors</td>\n",
       "      <td>3</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Pleased to support @amergateways at their Ju...</td>\n",
       "      <td>ImmigrantHeritageMonth</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   favorite_count                                          full_text  \\\n",
       "0               0  b\"RT @KUSINews: One of our longtime viewers wa...   \n",
       "1             258  b\"Today I'm urging the @CDCgov to immediately ...   \n",
       "2               0  b'Tomorrow, #MO03 seniors graduate from Calvar...   \n",
       "3               9  b'Congrats to #TeamUSA and Canton Native @JGre...   \n",
       "4               3  b'Pleased to support @amergateways at their Ju...   \n",
       "\n",
       "                 hashtags  retweet_count party_id  \n",
       "0                    KUSI             10        R  \n",
       "1             Coronavirus            111        R  \n",
       "2                    MO03              2        R  \n",
       "3    TeamUSA WorldJuniors              3        R  \n",
       "4  ImmigrantHeritageMonth              3        D  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('/Users/jacobjohnson/data_sets/congressional_tweet_training_data.csv', names=['favorite_count', 'full_text', 'hashtags', 'retweet_count', 'year', 'party_id'], \n",
    "                    # dtype={'favorite_count': int, 'full_text': str, 'hashtags': str, 'retweet_count': int, 'year': int, 'party_id': str}, \n",
    "                    skipinitialspace=True, skiprows=1, sep=',')\n",
    "\n",
    "test_df = pd.read_csv('/Users/jacobjohnson/data_sets/congressional_tweet_test_data.csv', names=['id', 'favorite_count_test', 'full_text_test', 'hashtags_test', 'retweet_count_test', 'year', 'party'], \n",
    "                    # dtype={'id': int, 'favorite_count': int, 'full_text': str, 'hashtags': str, 'retweet_count': int, 'year': int, 'party': str}, \n",
    "                    skipinitialspace=True, skiprows=1, sep=',')\n",
    "\n",
    "train_df.pop('year')\n",
    "test_df.pop('year')\n",
    "\n",
    "# test_df.head()\n",
    "train_df.head()\n",
    "\n",
    "# target variable\n",
    "# party = train_df.pop('party_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474242 training examples\n",
      "59280 validation examples\n",
      "59281 test examples\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "train, val, test = np.split(train_df.sample(frac=1), [int(0.8*len(train_df)), int(0.9*len(train_df))])\n",
    "\n",
    "print(len(train), 'training examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  df = dataframe.copy()\n",
    "  names = df.pop('party_id')\n",
    "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(df), names))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nf/brcqg8zx7xg31hjn_xbd2jc80000gp/T/ipykernel_32165/2315459290.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
      "/var/folders/nf/brcqg8zx7xg31hjn_xbd2jc80000gp/T/ipykernel_32165/2315459290.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
      "/var/folders/nf/brcqg8zx7xg31hjn_xbd2jc80000gp/T/ipykernel_32165/2315459290.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "\n",
    "# batch_size = 5\n",
    "# train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "\n",
    "batch_size = 256\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['favorite_count', 'full_text', 'hashtags', 'retweet_count', 'party_id']\n",
      "A batch of hashtags: tf.Tensor(\n",
      "[[b'va01']\n",
      " [b'TradeSummit2018']\n",
      " [b'TN01 iloveTNteachers NationalTeacherAppreciationDay']\n",
      " [b'ClimateChange']\n",
      " [b'BoomerSooner']\n",
      " [b'Bronx']\n",
      " [b'MadeInAmerica']\n",
      " [b'NorthDakota Bakken']\n",
      " [b'LA04 shopsmall SmallBusinessWeek']\n",
      " [b'MN03']\n",
      " [b'NetNeutrality OneMoreVote SaveTheInternet']\n",
      " [b'TaxBill']\n",
      " [b'SmallBusiness']\n",
      " [b'opioids Missouri']\n",
      " [b'PatFinucane UnifinishedBusiness']\n",
      " [b'Willowbrook CA44']\n",
      " [b'Haiyan']\n",
      " [b'EidMubarak']\n",
      " [b'SOTU Ag SpaceForce AL03']\n",
      " [b'CCE']\n",
      " [b'Texas America FridayFeelting tcot']\n",
      " [b'MobileOffice']\n",
      " [b'POTUS']\n",
      " [b'cosgrads2019']\n",
      " [b'ObamaCare ObamaCare KlineTTH']\n",
      " [b'PA06']\n",
      " [b'NY13 ForThePeople']\n",
      " [b'Obamacare']\n",
      " [b'ATF']\n",
      " [b'COVID19']\n",
      " [b'WA']\n",
      " [b'SWFL']\n",
      " [b'ProtectingOurInfantsAct']\n",
      " [b'FamiliesBelongTogether']\n",
      " [b'CIR']\n",
      " [b'BetterOffNow']\n",
      " [b'TrumpSwamp ForThePeople']\n",
      " [b'nationalprayerbreakfast']\n",
      " [b'AEW15chat']\n",
      " [b'FundForAmericasKidsAndGrandkids']\n",
      " [b's utpol']\n",
      " [b'Dreamers ProtectTheDream']\n",
      " [b'PA']\n",
      " [b'Everglades']\n",
      " [b'TaxCutsAndJobsAct']\n",
      " [b'LoveAnotherMother']\n",
      " [b'TaxDay']\n",
      " [b'COVID19']\n",
      " [b'SoFla']\n",
      " [b'NoFlyNoBuy CloseTheLoophole GunVote']\n",
      " [b'ScaliseStrong']\n",
      " [b'consciencerights religiousliberty HR1179 tcot']\n",
      " [b'WorldChildrensDay VoiceOfTomorrow']\n",
      " [b'AskDems DisarmHate']\n",
      " [b'NeverAgain']\n",
      " [b'LaborDay']\n",
      " [b'HomeIsHere']\n",
      " [b'CWA48']\n",
      " [b'ReligiousFreedomDay']\n",
      " [b'CensureTrump']\n",
      " [b'DrainTheSwamp']\n",
      " [b'ESSA']\n",
      " [b'DomesticViolenceAwarenessMonth']\n",
      " [b'NationalAgDay']\n",
      " [b'pancreaticcancer PANCaware']\n",
      " [b'Census2020']\n",
      " [b'OH5']\n",
      " [b'JOBSAct 4jobs tn03']\n",
      " [b'NCAABasketball15 MarchMadness']\n",
      " [b'MothersDay2019']\n",
      " [b'PhillyHighlight']\n",
      " [b'NoOneAboveTheLaw TrumpforSale']\n",
      " [b'floodinsurance']\n",
      " [b'13thAmendment']\n",
      " [b'ValentinesDay']\n",
      " [b'FlagDay']\n",
      " [b'NHjobstour fb']\n",
      " [b'SeniorFair Monday November AmericanLegion Summit']\n",
      " [b'taxreform']\n",
      " [b'WeekWithoutViolence2020']\n",
      " [b'Amtrak']\n",
      " [b'GoodFriday']\n",
      " [b'Minot']\n",
      " [b'WorldRefugeeDay']\n",
      " [b'NeverForget Renew911vcf']\n",
      " [b'FL15']\n",
      " [b'FollowTheFacts']\n",
      " [b'CampFire ActOnClimate']\n",
      " [b'HR356']\n",
      " [b'DC']\n",
      " [b'TownHall SavetheDate']\n",
      " [b'OpportunityZones PA09']\n",
      " [b'MerryChristmas']\n",
      " [b'NC01']\n",
      " [b'wvflood']\n",
      " [b'REINSAct']\n",
      " [b'LGBT EyesonChechnya']\n",
      " [b'OH14']\n",
      " [b'Aviation2050']\n",
      " [b'GetCovered']\n",
      " [b'VAWA']\n",
      " [b'OAA50']\n",
      " [b'DREAMAct immigration']\n",
      " [b'Trumpcare ACA ProtectOurCare']\n",
      " [b'NY22 4jobs']\n",
      " [b'RaiseTheWageAct ForThePeople']\n",
      " [b'SmallBiz Obamacare']\n",
      " [b'gapol']\n",
      " [b'RoseParade']\n",
      " [b'StandWithJohnLewis']\n",
      " [b'BadIranDeal Iran']\n",
      " [b'LongBranch']\n",
      " [b'USWNT']\n",
      " [b'PA8TTH']\n",
      " [b'IronRange MNMining']\n",
      " [b'SonomaCounty KincadeFire']\n",
      " [b'Ukraine Ukraine']\n",
      " [b'MuellerReport']\n",
      " [b'EarthDay EnvironmentalJustice ClimateCrisis EarthDay2020']\n",
      " [b'WomensMarch']\n",
      " [b'jobs']\n",
      " [b'HurricaneHarvey']\n",
      " [b'Iran']\n",
      " [b'AL03 CherokeeCo']\n",
      " [b'GOPbudget CancelGOPbudget ROC']\n",
      " [b'Marade MLKDay']\n",
      " [b'TX29']\n",
      " [b'derecho IA01 IA01']\n",
      " [b'SaveTitleX']\n",
      " [b'Covid_19']\n",
      " [b'DADT']\n",
      " [b'HappyMothersDay']\n",
      " [b'PPP']\n",
      " [b'EastTroublesomeFire']\n",
      " [b'NeverForget']\n",
      " [b'ValentinesDay']\n",
      " [b'TX31']\n",
      " [b'COVID19']\n",
      " [b'FatBearWeek Alaska']\n",
      " [b'MagicMonday']\n",
      " [b'PearlHabor75']\n",
      " [b'rolltribe']\n",
      " [b'MothersDay']\n",
      " [b'BenghaziReport']\n",
      " [b'renewableenergy jobs REpolicy']\n",
      " [b'ARCAOrganics']\n",
      " [b'AR3']\n",
      " [b'ABLE taxreform']\n",
      " [b'Iran']\n",
      " [b'Coronavirus']\n",
      " [b'VAMissionAct VAMissionAct']\n",
      " [b'Coronavirus']\n",
      " [b'NE03']\n",
      " [b'SuddenCardiacArrest HR4152']\n",
      " [b'SupportTEAPSPA']\n",
      " [b'SWMO']\n",
      " [b'Friday Ohio OH15']\n",
      " [b'FCM12']\n",
      " [b'NewYork Census']\n",
      " [b'Benghazi VA IranDeal']\n",
      " [b'climatechange']\n",
      " [b'ABLEAct']\n",
      " [b'president SOTU']\n",
      " [b'MS01']\n",
      " [b'4jobs']\n",
      " [b'ACARepeal ProtectOurCare']\n",
      " [b'Nebraska']\n",
      " [b'Cuba Venezuela China humanrights Israel UN']\n",
      " [b'SOTU2019']\n",
      " [b'ConsumersFirst ProtectConsumers']\n",
      " [b'BattleBornUpdate']\n",
      " [b'Bronx']\n",
      " [b'SecureElectionsAct']\n",
      " [b'Cookeville Emmy']\n",
      " [b'HurricaneHarvey']\n",
      " [b'JusticeInPolicing']\n",
      " [b'RITownHall']\n",
      " [b'KS03']\n",
      " [b'SOTU']\n",
      " [b'Trumpcare']\n",
      " [b'SMTTT']\n",
      " [b'BlackHistoryMonth']\n",
      " [b'disappointing']\n",
      " [b'Route66']\n",
      " [b'ClimateChange']\n",
      " [b'renewables sustainability']\n",
      " [b'twill']\n",
      " [b'cuomoprimetime']\n",
      " [b'ProtectSeniors']\n",
      " [b'NatSecForum']\n",
      " [b'ButchLewisAct retirement']\n",
      " [b'NoBorderWall']\n",
      " [b'Obamacare Gruber']\n",
      " [b'HolocaustRemembranceDay NeverAgain']\n",
      " [b'EndInfanticide']\n",
      " [b'MothersDay']\n",
      " [b'HR1700']\n",
      " [b'InternationalWomensDay']\n",
      " [b'SMH']\n",
      " [b'Gitmo']\n",
      " [b'PoetryMonth']\n",
      " [b'Dolphins']\n",
      " [b'Military Naval Air Merchant']\n",
      " [b'GlobalGagRule']\n",
      " [b'DisasterRelief']\n",
      " [b'MissouriForestProducts MO8']\n",
      " [b'WomenSucceed']\n",
      " [b'PreparingFutureFarmers WA04']\n",
      " [b'TaxDay IA03']\n",
      " [b'COVID19']\n",
      " [b'VAaccountability']\n",
      " [b'NoHB1057']\n",
      " [b'GreenNewDeal vtpoli']\n",
      " [b'AR3 USMCA']\n",
      " [b'POWMIA']\n",
      " [b'TeamDutch MD02 KeepingGirlsInSchool Didyouknow']\n",
      " [b'MO']\n",
      " [b'TrumpBudget']\n",
      " [b'Alzheimers research caregivers']\n",
      " [b'NationalPoliceWeek']\n",
      " [b'FBI']\n",
      " [b'ak']\n",
      " [b'Michigan']\n",
      " [b'milspouses vets']\n",
      " [b'MOSen tcot']\n",
      " [b'netDE']\n",
      " [b'CA11 equalpaynow']\n",
      " [b'SESTA']\n",
      " [b'nationaldayofprayer']\n",
      " [b'MS01']\n",
      " [b'coronavirus']\n",
      " [b'KS04']\n",
      " [b'MonumentalMonday']\n",
      " [b'freespeech']\n",
      " [b'IA03']\n",
      " [b'vlog']\n",
      " [b'healthcare']\n",
      " [b'Ramadan']\n",
      " [b'TX']\n",
      " [b'Harvey']\n",
      " [b'crosshairs bullets']\n",
      " [b'LeadOnTrade']\n",
      " [b'taxreformnow taxreform']\n",
      " [b'USMCA AR3']\n",
      " [b'TimeIsNow CIR']\n",
      " [b'netneutrality']\n",
      " [b'WRDA Senate']\n",
      " [b'cafecito FL27']\n",
      " [b'COVID19']\n",
      " [b'ExcelsiorSpring MissouriSpotlight']\n",
      " [b'veteran']\n",
      " [b'agriculture IoT']\n",
      " [b'FreePastorBrunson']\n",
      " [b'Lowell MA3']\n",
      " [b'WomensHistoryMonth']\n",
      " [b'Ortega Nicaragua paz democracia']], shape=(256, 1), dtype=string)\n",
      "A batch of targets: tf.Tensor(\n",
      "[b'R' b'R' b'R' b'D' b'R' b'D' b'R' b'R' b'R' b'R' b'D' b'D' b'R' b'D'\n",
      " b'D' b'D' b'D' b'D' b'R' b'D' b'R' b'R' b'D' b'R' b'R' b'D' b'D' b'R'\n",
      " b'R' b'R' b'D' b'R' b'R' b'D' b'D' b'R' b'D' b'R' b'D' b'R' b'R' b'D'\n",
      " b'R' b'R' b'R' b'R' b'R' b'D' b'R' b'D' b'R' b'R' b'D' b'D' b'R' b'R'\n",
      " b'D' b'D' b'R' b'D' b'R' b'D' b'D' b'D' b'R' b'D' b'R' b'R' b'R' b'R'\n",
      " b'D' b'D' b'R' b'D' b'R' b'D' b'R' b'D' b'D' b'D' b'D' b'D' b'R' b'D'\n",
      " b'D' b'R' b'D' b'D' b'D' b'R' b'D' b'R' b'R' b'D' b'R' b'R' b'D' b'R'\n",
      " b'D' b'D' b'D' b'D' b'D' b'D' b'R' b'D' b'R' b'R' b'R' b'D' b'R' b'D'\n",
      " b'D' b'R' b'R' b'D' b'R' b'D' b'D' b'D' b'R' b'D' b'R' b'R' b'D' b'D'\n",
      " b'D' b'D' b'D' b'R' b'R' b'R' b'R' b'D' b'D' b'R' b'R' b'D' b'R' b'D'\n",
      " b'D' b'R' b'D' b'R' b'D' b'D' b'R' b'R' b'R' b'R' b'R' b'D' b'R' b'R'\n",
      " b'D' b'R' b'R' b'R' b'D' b'R' b'D' b'D' b'R' b'R' b'R' b'D' b'R' b'R'\n",
      " b'D' b'D' b'D' b'D' b'D' b'R' b'D' b'D' b'D' b'D' b'R' b'D' b'R' b'D'\n",
      " b'R' b'R' b'D' b'D' b'D' b'D' b'D' b'D' b'D' b'D' b'R' b'R' b'R' b'D'\n",
      " b'R' b'D' b'D' b'D' b'D' b'R' b'D' b'D' b'D' b'R' b'D' b'R' b'D' b'D'\n",
      " b'R' b'D' b'D' b'R' b'R' b'D' b'R' b'D' b'D' b'R' b'R' b'D' b'D' b'D'\n",
      " b'R' b'D' b'D' b'D' b'R' b'R' b'D' b'R' b'D' b'R' b'R' b'D' b'D' b'D'\n",
      " b'R' b'R' b'D' b'D' b'R' b'R' b'D' b'D' b'R' b'D' b'D' b'D' b'D' b'R'\n",
      " b'R' b'D' b'R' b'R'], shape=(256,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "\n",
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print('Every feature:', list(train_features.keys()))\n",
    "print('A batch of hashtags:', train_features['hashtags'])\n",
    "print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "  normalizer = layers.Normalization(axis=None)\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  normalizer.adapt(feature_ds)\n",
    "  return normalizer\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  if dtype == 'string':\n",
    "    # DOES THIS REQUIRE ALL COLUMNS HAVE DTYPES??\n",
    "    index = layers.StringLookup(max_tokens=max_tokens)\n",
    "  else:\n",
    "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  index.adapt(feature_ds)\n",
    "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "  return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "\n",
    "# Numerical features.\n",
    "\n",
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "for header in ['favorite_count', 'retweet_count']:\n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)\n",
    "  encoded_features.append(encoded_numeric_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "\n",
    "categorical_cols = ['full_text', 'hashtags', 'party_id']\n",
    "\n",
    "for header in categorical_cols:\n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(name=header,\n",
    "                                               dataset=train_ds,\n",
    "                                               dtype='string',\n",
    "                                               max_tokens=None)\n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features.append(encoded_categorical_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "\n",
    "party_id_col = tf.keras.Input(shape=(1,), name='party_id', dtype='string')\n",
    "\n",
    "encoding_layer = get_category_encoding_layer(name='party_id',\n",
    "                                             dataset=train_ds,\n",
    "                                             dtype='string',\n",
    "                                             max_tokens=None)\n",
    "encoded_party_id_col = encoding_layer(party_id_col)\n",
    "all_inputs.append(party_id_col)\n",
    "encoded_features.append(encoded_party_id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11\n",
    "\n",
    "test_party_id_col = train_features['party_id']\n",
    "test_party_id_layer = get_category_encoding_layer(name='party_id',\n",
    "                                             dataset=train_ds,\n",
    "                                             dtype='string',\n",
    "                                             max_tokens=None)\n",
    "test_party_id_layer(test_party_id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 5018), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12\n",
    " \n",
    "test_retweet_count_col = train_features['retweet_count']\n",
    "test_retweet_count_layer = get_category_encoding_layer(name='retweet_count', dataset=train_ds, dtype='int')\n",
    "test_retweet_count_layer(test_retweet_count_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 7918), dtype=float32, numpy=\n",
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13\n",
    "\n",
    "test_favorite_count_col = train_features['favorite_count']\n",
    "test_favorite_count_layer = get_category_encoding_layer(name='favorite_count', dataset=train_ds, dtype='int')\n",
    "test_favorite_count_layer(test_favorite_count_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 470945), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14\n",
    "\n",
    "test_full_text_col = train_features['full_text']\n",
    "test_full_text_layer = get_category_encoding_layer(name='full_text', dataset=train_ds, dtype='string')\n",
    "test_full_text_layer(test_full_text_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 158956), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15\n",
    "\n",
    "test_hashtags_col = train_features['hashtags']\n",
    "test_hashtags_layer = get_category_encoding_layer(name='hashtags', dataset=train_ds, dtype='string')\n",
    "test_hashtags_layer(test_hashtags_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16\n",
    "\n",
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['party_id'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "2022-05-06 10:30:15.593547: W tensorflow/core/framework/op_kernel.cc:1722] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/nf/brcqg8zx7xg31hjn_xbd2jc80000gp/T/ipykernel_32165/2809293729.py\", line 1, in <cell line: 1>\n      model.fit(train_ds, epochs=10, validation_data=val_ds)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 918, in compute_loss\n      return self.compiled_loss(\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/losses.py\", line 1922, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_403675]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/nf/brcqg8zx7xg31hjn_xbd2jc80000gp/T/ipykernel_32165/2809293729.py\", line 1, in <cell line: 1>\n      model.fit(train_ds, epochs=10, validation_data=val_ds)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 918, in compute_loss\n      return self.compiled_loss(\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/losses.py\", line 1922, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_403675]"
     ]
    }
   ],
   "source": [
    "# 18\n",
    "\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
