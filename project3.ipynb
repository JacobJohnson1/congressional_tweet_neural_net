{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup \n",
    "from keras import layers\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>party_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"RT @KUSINews: One of our longtime viewers wa...</td>\n",
       "      <td>KUSI</td>\n",
       "      <td>10</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>b\"Today I'm urging the @CDCgov to immediately ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>111</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Tomorrow, #MO03 seniors graduate from Calvar...</td>\n",
       "      <td>MO03</td>\n",
       "      <td>2</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>b'Congrats to #TeamUSA and Canton Native @JGre...</td>\n",
       "      <td>TeamUSA WorldJuniors</td>\n",
       "      <td>3</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Pleased to support @amergateways at their Ju...</td>\n",
       "      <td>ImmigrantHeritageMonth</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   favorite_count                                          full_text  \\\n",
       "0               0  b\"RT @KUSINews: One of our longtime viewers wa...   \n",
       "1             258  b\"Today I'm urging the @CDCgov to immediately ...   \n",
       "2               0  b'Tomorrow, #MO03 seniors graduate from Calvar...   \n",
       "3               9  b'Congrats to #TeamUSA and Canton Native @JGre...   \n",
       "4               3  b'Pleased to support @amergateways at their Ju...   \n",
       "\n",
       "                 hashtags  retweet_count party_id  \n",
       "0                    KUSI             10        R  \n",
       "1             Coronavirus            111        R  \n",
       "2                    MO03              2        R  \n",
       "3    TeamUSA WorldJuniors              3        R  \n",
       "4  ImmigrantHeritageMonth              3        D  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('/Users/jacobjohnson/data_sets/congressional_tweet_training_data.csv', names=['favorite_count', 'full_text', 'hashtags', 'retweet_count', 'year', 'party_id'], \n",
    "                    # dtype={'favorite_count': int, 'full_text': str, 'hashtags': str, 'retweet_count': int, 'year': int, 'party_id': str}, \n",
    "                    skipinitialspace=True, skiprows=1, sep=',')\n",
    "\n",
    "test_df = pd.read_csv('/Users/jacobjohnson/data_sets/congressional_tweet_test_data.csv', names=['id', 'favorite_count_test', 'full_text_test', 'hashtags_test', 'retweet_count_test', 'year', 'party'], \n",
    "                    # dtype={'id': int, 'favorite_count': int, 'full_text': str, 'hashtags': str, 'retweet_count': int, 'year': int, 'party': str}, \n",
    "                    skipinitialspace=True, skiprows=1, sep=',')\n",
    "\n",
    "train_df.pop('year')\n",
    "test_df.pop('year')\n",
    "\n",
    "# test_df.head()\n",
    "train_df.head()\n",
    "\n",
    "# target variable\n",
    "# party = train_df.pop('party_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474242 training examples\n",
      "59280 validation examples\n",
      "59281 test examples\n"
     ]
    }
   ],
   "source": [
    "train, val, test = np.split(train_df.sample(frac=1), [int(0.8*len(train_df)), int(0.9*len(train_df))])\n",
    "\n",
    "print(len(train), 'training examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  df = dataframe.copy()\n",
    "  names = df.pop('party_id')\n",
    "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(df), names))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nf/brcqg8zx7xg31hjn_xbd2jc80000gp/T/ipykernel_32165/2315459290.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
      "/var/folders/nf/brcqg8zx7xg31hjn_xbd2jc80000gp/T/ipykernel_32165/2315459290.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
      "/var/folders/nf/brcqg8zx7xg31hjn_xbd2jc80000gp/T/ipykernel_32165/2315459290.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 5\n",
    "# train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "\n",
    "batch_size = 256\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['favorite_count', 'full_text', 'hashtags', 'retweet_count', 'party_id']\n",
      "A batch of hashtags: tf.Tensor(\n",
      "[[b'va01']\n",
      " [b'TradeSummit2018']\n",
      " [b'TN01 iloveTNteachers NationalTeacherAppreciationDay']\n",
      " [b'ClimateChange']\n",
      " [b'BoomerSooner']\n",
      " [b'Bronx']\n",
      " [b'MadeInAmerica']\n",
      " [b'NorthDakota Bakken']\n",
      " [b'LA04 shopsmall SmallBusinessWeek']\n",
      " [b'MN03']\n",
      " [b'NetNeutrality OneMoreVote SaveTheInternet']\n",
      " [b'TaxBill']\n",
      " [b'SmallBusiness']\n",
      " [b'opioids Missouri']\n",
      " [b'PatFinucane UnifinishedBusiness']\n",
      " [b'Willowbrook CA44']\n",
      " [b'Haiyan']\n",
      " [b'EidMubarak']\n",
      " [b'SOTU Ag SpaceForce AL03']\n",
      " [b'CCE']\n",
      " [b'Texas America FridayFeelting tcot']\n",
      " [b'MobileOffice']\n",
      " [b'POTUS']\n",
      " [b'cosgrads2019']\n",
      " [b'ObamaCare ObamaCare KlineTTH']\n",
      " [b'PA06']\n",
      " [b'NY13 ForThePeople']\n",
      " [b'Obamacare']\n",
      " [b'ATF']\n",
      " [b'COVID19']\n",
      " [b'WA']\n",
      " [b'SWFL']\n",
      " [b'ProtectingOurInfantsAct']\n",
      " [b'FamiliesBelongTogether']\n",
      " [b'CIR']\n",
      " [b'BetterOffNow']\n",
      " [b'TrumpSwamp ForThePeople']\n",
      " [b'nationalprayerbreakfast']\n",
      " [b'AEW15chat']\n",
      " [b'FundForAmericasKidsAndGrandkids']\n",
      " [b's utpol']\n",
      " [b'Dreamers ProtectTheDream']\n",
      " [b'PA']\n",
      " [b'Everglades']\n",
      " [b'TaxCutsAndJobsAct']\n",
      " [b'LoveAnotherMother']\n",
      " [b'TaxDay']\n",
      " [b'COVID19']\n",
      " [b'SoFla']\n",
      " [b'NoFlyNoBuy CloseTheLoophole GunVote']\n",
      " [b'ScaliseStrong']\n",
      " [b'consciencerights religiousliberty HR1179 tcot']\n",
      " [b'WorldChildrensDay VoiceOfTomorrow']\n",
      " [b'AskDems DisarmHate']\n",
      " [b'NeverAgain']\n",
      " [b'LaborDay']\n",
      " [b'HomeIsHere']\n",
      " [b'CWA48']\n",
      " [b'ReligiousFreedomDay']\n",
      " [b'CensureTrump']\n",
      " [b'DrainTheSwamp']\n",
      " [b'ESSA']\n",
      " [b'DomesticViolenceAwarenessMonth']\n",
      " [b'NationalAgDay']\n",
      " [b'pancreaticcancer PANCaware']\n",
      " [b'Census2020']\n",
      " [b'OH5']\n",
      " [b'JOBSAct 4jobs tn03']\n",
      " [b'NCAABasketball15 MarchMadness']\n",
      " [b'MothersDay2019']\n",
      " [b'PhillyHighlight']\n",
      " [b'NoOneAboveTheLaw TrumpforSale']\n",
      " [b'floodinsurance']\n",
      " [b'13thAmendment']\n",
      " [b'ValentinesDay']\n",
      " [b'FlagDay']\n",
      " [b'NHjobstour fb']\n",
      " [b'SeniorFair Monday November AmericanLegion Summit']\n",
      " [b'taxreform']\n",
      " [b'WeekWithoutViolence2020']\n",
      " [b'Amtrak']\n",
      " [b'GoodFriday']\n",
      " [b'Minot']\n",
      " [b'WorldRefugeeDay']\n",
      " [b'NeverForget Renew911vcf']\n",
      " [b'FL15']\n",
      " [b'FollowTheFacts']\n",
      " [b'CampFire ActOnClimate']\n",
      " [b'HR356']\n",
      " [b'DC']\n",
      " [b'TownHall SavetheDate']\n",
      " [b'OpportunityZones PA09']\n",
      " [b'MerryChristmas']\n",
      " [b'NC01']\n",
      " [b'wvflood']\n",
      " [b'REINSAct']\n",
      " [b'LGBT EyesonChechnya']\n",
      " [b'OH14']\n",
      " [b'Aviation2050']\n",
      " [b'GetCovered']\n",
      " [b'VAWA']\n",
      " [b'OAA50']\n",
      " [b'DREAMAct immigration']\n",
      " [b'Trumpcare ACA ProtectOurCare']\n",
      " [b'NY22 4jobs']\n",
      " [b'RaiseTheWageAct ForThePeople']\n",
      " [b'SmallBiz Obamacare']\n",
      " [b'gapol']\n",
      " [b'RoseParade']\n",
      " [b'StandWithJohnLewis']\n",
      " [b'BadIranDeal Iran']\n",
      " [b'LongBranch']\n",
      " [b'USWNT']\n",
      " [b'PA8TTH']\n",
      " [b'IronRange MNMining']\n",
      " [b'SonomaCounty KincadeFire']\n",
      " [b'Ukraine Ukraine']\n",
      " [b'MuellerReport']\n",
      " [b'EarthDay EnvironmentalJustice ClimateCrisis EarthDay2020']\n",
      " [b'WomensMarch']\n",
      " [b'jobs']\n",
      " [b'HurricaneHarvey']\n",
      " [b'Iran']\n",
      " [b'AL03 CherokeeCo']\n",
      " [b'GOPbudget CancelGOPbudget ROC']\n",
      " [b'Marade MLKDay']\n",
      " [b'TX29']\n",
      " [b'derecho IA01 IA01']\n",
      " [b'SaveTitleX']\n",
      " [b'Covid_19']\n",
      " [b'DADT']\n",
      " [b'HappyMothersDay']\n",
      " [b'PPP']\n",
      " [b'EastTroublesomeFire']\n",
      " [b'NeverForget']\n",
      " [b'ValentinesDay']\n",
      " [b'TX31']\n",
      " [b'COVID19']\n",
      " [b'FatBearWeek Alaska']\n",
      " [b'MagicMonday']\n",
      " [b'PearlHabor75']\n",
      " [b'rolltribe']\n",
      " [b'MothersDay']\n",
      " [b'BenghaziReport']\n",
      " [b'renewableenergy jobs REpolicy']\n",
      " [b'ARCAOrganics']\n",
      " [b'AR3']\n",
      " [b'ABLE taxreform']\n",
      " [b'Iran']\n",
      " [b'Coronavirus']\n",
      " [b'VAMissionAct VAMissionAct']\n",
      " [b'Coronavirus']\n",
      " [b'NE03']\n",
      " [b'SuddenCardiacArrest HR4152']\n",
      " [b'SupportTEAPSPA']\n",
      " [b'SWMO']\n",
      " [b'Friday Ohio OH15']\n",
      " [b'FCM12']\n",
      " [b'NewYork Census']\n",
      " [b'Benghazi VA IranDeal']\n",
      " [b'climatechange']\n",
      " [b'ABLEAct']\n",
      " [b'president SOTU']\n",
      " [b'MS01']\n",
      " [b'4jobs']\n",
      " [b'ACARepeal ProtectOurCare']\n",
      " [b'Nebraska']\n",
      " [b'Cuba Venezuela China humanrights Israel UN']\n",
      " [b'SOTU2019']\n",
      " [b'ConsumersFirst ProtectConsumers']\n",
      " [b'BattleBornUpdate']\n",
      " [b'Bronx']\n",
      " [b'SecureElectionsAct']\n",
      " [b'Cookeville Emmy']\n",
      " [b'HurricaneHarvey']\n",
      " [b'JusticeInPolicing']\n",
      " [b'RITownHall']\n",
      " [b'KS03']\n",
      " [b'SOTU']\n",
      " [b'Trumpcare']\n",
      " [b'SMTTT']\n",
      " [b'BlackHistoryMonth']\n",
      " [b'disappointing']\n",
      " [b'Route66']\n",
      " [b'ClimateChange']\n",
      " [b'renewables sustainability']\n",
      " [b'twill']\n",
      " [b'cuomoprimetime']\n",
      " [b'ProtectSeniors']\n",
      " [b'NatSecForum']\n",
      " [b'ButchLewisAct retirement']\n",
      " [b'NoBorderWall']\n",
      " [b'Obamacare Gruber']\n",
      " [b'HolocaustRemembranceDay NeverAgain']\n",
      " [b'EndInfanticide']\n",
      " [b'MothersDay']\n",
      " [b'HR1700']\n",
      " [b'InternationalWomensDay']\n",
      " [b'SMH']\n",
      " [b'Gitmo']\n",
      " [b'PoetryMonth']\n",
      " [b'Dolphins']\n",
      " [b'Military Naval Air Merchant']\n",
      " [b'GlobalGagRule']\n",
      " [b'DisasterRelief']\n",
      " [b'MissouriForestProducts MO8']\n",
      " [b'WomenSucceed']\n",
      " [b'PreparingFutureFarmers WA04']\n",
      " [b'TaxDay IA03']\n",
      " [b'COVID19']\n",
      " [b'VAaccountability']\n",
      " [b'NoHB1057']\n",
      " [b'GreenNewDeal vtpoli']\n",
      " [b'AR3 USMCA']\n",
      " [b'POWMIA']\n",
      " [b'TeamDutch MD02 KeepingGirlsInSchool Didyouknow']\n",
      " [b'MO']\n",
      " [b'TrumpBudget']\n",
      " [b'Alzheimers research caregivers']\n",
      " [b'NationalPoliceWeek']\n",
      " [b'FBI']\n",
      " [b'ak']\n",
      " [b'Michigan']\n",
      " [b'milspouses vets']\n",
      " [b'MOSen tcot']\n",
      " [b'netDE']\n",
      " [b'CA11 equalpaynow']\n",
      " [b'SESTA']\n",
      " [b'nationaldayofprayer']\n",
      " [b'MS01']\n",
      " [b'coronavirus']\n",
      " [b'KS04']\n",
      " [b'MonumentalMonday']\n",
      " [b'freespeech']\n",
      " [b'IA03']\n",
      " [b'vlog']\n",
      " [b'healthcare']\n",
      " [b'Ramadan']\n",
      " [b'TX']\n",
      " [b'Harvey']\n",
      " [b'crosshairs bullets']\n",
      " [b'LeadOnTrade']\n",
      " [b'taxreformnow taxreform']\n",
      " [b'USMCA AR3']\n",
      " [b'TimeIsNow CIR']\n",
      " [b'netneutrality']\n",
      " [b'WRDA Senate']\n",
      " [b'cafecito FL27']\n",
      " [b'COVID19']\n",
      " [b'ExcelsiorSpring MissouriSpotlight']\n",
      " [b'veteran']\n",
      " [b'agriculture IoT']\n",
      " [b'FreePastorBrunson']\n",
      " [b'Lowell MA3']\n",
      " [b'WomensHistoryMonth']\n",
      " [b'Ortega Nicaragua paz democracia']], shape=(256, 1), dtype=string)\n",
      "A batch of targets: tf.Tensor(\n",
      "[b'R' b'R' b'R' b'D' b'R' b'D' b'R' b'R' b'R' b'R' b'D' b'D' b'R' b'D'\n",
      " b'D' b'D' b'D' b'D' b'R' b'D' b'R' b'R' b'D' b'R' b'R' b'D' b'D' b'R'\n",
      " b'R' b'R' b'D' b'R' b'R' b'D' b'D' b'R' b'D' b'R' b'D' b'R' b'R' b'D'\n",
      " b'R' b'R' b'R' b'R' b'R' b'D' b'R' b'D' b'R' b'R' b'D' b'D' b'R' b'R'\n",
      " b'D' b'D' b'R' b'D' b'R' b'D' b'D' b'D' b'R' b'D' b'R' b'R' b'R' b'R'\n",
      " b'D' b'D' b'R' b'D' b'R' b'D' b'R' b'D' b'D' b'D' b'D' b'D' b'R' b'D'\n",
      " b'D' b'R' b'D' b'D' b'D' b'R' b'D' b'R' b'R' b'D' b'R' b'R' b'D' b'R'\n",
      " b'D' b'D' b'D' b'D' b'D' b'D' b'R' b'D' b'R' b'R' b'R' b'D' b'R' b'D'\n",
      " b'D' b'R' b'R' b'D' b'R' b'D' b'D' b'D' b'R' b'D' b'R' b'R' b'D' b'D'\n",
      " b'D' b'D' b'D' b'R' b'R' b'R' b'R' b'D' b'D' b'R' b'R' b'D' b'R' b'D'\n",
      " b'D' b'R' b'D' b'R' b'D' b'D' b'R' b'R' b'R' b'R' b'R' b'D' b'R' b'R'\n",
      " b'D' b'R' b'R' b'R' b'D' b'R' b'D' b'D' b'R' b'R' b'R' b'D' b'R' b'R'\n",
      " b'D' b'D' b'D' b'D' b'D' b'R' b'D' b'D' b'D' b'D' b'R' b'D' b'R' b'D'\n",
      " b'R' b'R' b'D' b'D' b'D' b'D' b'D' b'D' b'D' b'D' b'R' b'R' b'R' b'D'\n",
      " b'R' b'D' b'D' b'D' b'D' b'R' b'D' b'D' b'D' b'R' b'D' b'R' b'D' b'D'\n",
      " b'R' b'D' b'D' b'R' b'R' b'D' b'R' b'D' b'D' b'R' b'R' b'D' b'D' b'D'\n",
      " b'R' b'D' b'D' b'D' b'R' b'R' b'D' b'R' b'D' b'R' b'R' b'D' b'D' b'D'\n",
      " b'R' b'R' b'D' b'D' b'R' b'R' b'D' b'D' b'R' b'D' b'D' b'D' b'D' b'R'\n",
      " b'R' b'D' b'R' b'R'], shape=(256,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print('Every feature:', list(train_features.keys()))\n",
    "print('A batch of hashtags:', train_features['hashtags'])\n",
    "print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for data preprocessing\n",
    "# these have not been tailored to my data\n",
    "\n",
    "# may use this one retweet_count & favorite_count\n",
    "def get_normalization_layer(name, dataset):\n",
    "  normalizer = layers.Normalization(axis=None)\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  normalizer.adapt(feature_ds)\n",
    "  return normalizer\n",
    "\n",
    "# may use this on full_text & hashtags\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  if dtype == 'string':\n",
    "    # DO THESE REQUIRE ALL COLUMNS HAVE DTYPES??\n",
    "    index = layers.StringLookup(max_tokens=max_tokens)\n",
    "  else:\n",
    "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  index.adapt(feature_ds)\n",
    "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "  return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 5018), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_retweet_count_col = train_features['retweet_count']\n",
    "test_retweet_count_layer = get_category_encoding_layer(name='retweet_count', dataset=train_ds, dtype='int')\n",
    "test_retweet_count_layer(test_retweet_count_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 7918), dtype=float32, numpy=\n",
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_favorite_count_col = train_features['favorite_count']\n",
    "test_favorite_count_layer = get_category_encoding_layer(name='favorite_count', dataset=train_ds, dtype='int')\n",
    "test_favorite_count_layer(test_favorite_count_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 470945), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_full_text_col = train_features['full_text']\n",
    "test_full_text_layer = get_category_encoding_layer(name='full_text', dataset=train_ds, dtype='string')\n",
    "test_full_text_layer(test_full_text_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 158956), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hashtags_col = train_features['hashtags']\n",
    "test_hashtags_layer = get_category_encoding_layer(name='hashtags', dataset=train_ds, dtype='string')\n",
    "test_hashtags_layer(test_hashtags_col)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
