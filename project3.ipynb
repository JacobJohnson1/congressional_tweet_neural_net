{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup \n",
    "from keras import layers\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "favorite_count                                                  258\n",
      "full_text         b\"Today I'm urging the @CDCgov to immediately ...\n",
      "hashtags                                                Coronavirus\n",
      "retweet_count                                                   111\n",
      "year                                                         2020.0\n",
      "party_id                                                          R\n",
      "Name: 1, dtype: object\n",
      "Index(['favorite_count', 'full_text', 'hashtags', 'retweet_count', 'year',\n",
      "       'party_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filepath_dict = {'trainingData' : '/Users/jacobjohnson/data_sets/congressional_tweet_training_data.csv', 'testData' : '/Users/jacobjohnson/data_sets/congressional_tweet_test_data.csv'}\n",
    "\n",
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():\n",
    "    df = pd.read_csv(filepath, names=['favorite_count', 'full_text', 'hashtags', 'retweet_count', 'year', 'party_id'], skipinitialspace=True, skiprows=1, sep=',')\n",
    "    # df['source'] = source\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "print(df.iloc[1])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_training = df\n",
    "\n",
    "hashtags = df_training['hashtags'].values\n",
    "y = df_training['party_id'].values\n",
    "\n",
    "# ds_split, info = tfds.load('trainingData', split=['train[:20%]', 'train[20%:]'], as_supervised=True, with_info=True)\n",
    "\n",
    "\n",
    "hashtags_train, hashtags_test, y_train, y_test = train_test_split(hashtags, y, test_size=0.25, random_state=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "training_data = np.array(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<643352x79894 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 948308 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(hashtags_train)\n",
    "\n",
    "X_train = vectorizer.transform(hashtags_train)\n",
    "X_test  = vectorizer.transform(hashtags_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7679190118022299\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                798950    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 799,082\n",
      "Trainable params: 799,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:18:16.818699: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "  tf.keras.layers.Dense(10, input_dim=input_dim, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# input_tensor = tf.convert_to_tensor(training_data)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# # input_tensor = tf.constant(df)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# predictions = model(input_tensor)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# predictions[:5]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# tf.nn.softmax(predictions[:5])\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train_features \u001b[38;5;241m=\u001b[39m training_data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 9\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparty_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m training_data\u001b[38;5;241m.\u001b[39mfit(train_features, train_labels, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# input_tensor = tf.convert_to_tensor(training_data)\n",
    "# # input_tensor = tf.constant(df)\n",
    "\n",
    "# predictions = model(input_tensor)\n",
    "# predictions[:5]\n",
    "# tf.nn.softmax(predictions[:5])\n",
    "\n",
    "train_features = training_data.copy()\n",
    "train_labels = train_features['party_id']\n",
    "\n",
    "\n",
    "training_data.fit(train_features, train_labels, epochs=10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
